main: 
  random_state: 10
  batch_size: 1
  epochs: 40
  buffer_size: 50 # Size of the buffer to store generated images

data:
  data_part: 0.9 # Proportions to split data for train and evaluation
  sub_part_data: 1.0 # Part of picked data to use for training (useful for project tests, for real training process set == 1)
  size: [256, 256]
  mean: [0.485, 0.456, 0.406]
  std: [0.229, 0.224, 0.225]

distributed:
  world_size: 1  # Number of GPU/CPU
  # Initialization the default distributed process group
  backend: 'nccl' # 'nccl' -- for GPU, 'gloo' for CPU
  port: '12345'
  address: 'localhost'

optimizers:
  gen:
    lr: 0.00002
    betas: [0.5, 0.999]
  discA:
    lr: 0.00002
    betas: [0.5, 0.999]
  discB:
    lr: 0.00002
    betas: [0.5, 0.999]

models:
  # If pretrained model wasn't loaded 
  # Initialze model weights with Gaussian distribution N(0, 0.2)
  mean: 0.0
  std: 0.2

loss:
  adversarial:
    ltype: 'MSE'
    real_val: 1.0
    fake_val: 0.0
    adv_alpha: 0.5

  cycle:
    ltype: 'L1'
    lambda_A: 10.0
    lambda_B: 10.0

  identity:
    ltype: 'L1'
    lambda_idn: 0.5
