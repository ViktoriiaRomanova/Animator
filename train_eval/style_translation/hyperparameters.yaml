main: 
  random_state: 10
  batch_size: 64
  epochs: 2
  buffer_size: 50 # Size of the buffer to store generated images

data:
  data_part: 0.9 # Proportions to split data for train and evaluation
  sub_part_data: 1.0 # Part of picked data to use for training (useful for project tests, for real training process set == 1)

distributed:
  world_size: 2  # Number of GPU/CPU
  # Initialization the default distributed process group
  backend: 'nccl' # 'nccl' -- for GPU, 'gloo' for CPU
  port: '12345'
  address: 'localhost'

optimizers:
  gen:
    lr: 0.0002
    betas: [0.5, 0.999]
  discA:
    lr: 0.0002
    betas: [0.5, 0.999]
  discB:
    lr: 0.0002
    betas: [0.5, 0.999]
    

models:
  # If pretrained model wasn't loaded 
  # Initialze model weights with Gaussian distribution N(0, 0.2)
  mean: 0.0
  std: 0.2
