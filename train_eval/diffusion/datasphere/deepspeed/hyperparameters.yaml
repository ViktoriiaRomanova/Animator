main:
  random_state: 10
  epochs: 1
  buffer_size: 50 # Size of the buffer to store generated images
  caption_forward: "person to anime character"
  caption_reverse: "anime character to person"
  segmentation_model: "/workspace/trained_models/segmentation/99.pt"
  segmentation_model_type: "B" # A, B OR C (details in figure_extraction unet model description)
  save_step: 1 # Once in save_step epoch save model weights
  warm_up: 1000 # number of steps without application of the segmentation 

data:
  data_part: 0.9 # Proportions to split data for train and evaluation
  sub_part_data: 0.9 # Part of picked data to use for training (useful for project tests, for real training process set == 1)
  size: [512, 512]
  mean: [0.5, 0.5, 0.5]
  std: [0.5, 0.5, 0.5]

generator:
  unet_lora_rank: 128
  vae_lora_rank: 4
  gamma: 1

discriminator:
  cv_type: "clip"
  loss_type: "multilevel_sigmoid"

loss:
  adversarial:
    adv_alpha: 0.5

  cycle:
    ltype: "L1"
    lambda_A: 1.0
    lambda_B: 1.0
    lambda_lpips: 10.0

  identity:
    ltype: "L1"
    lambda_idn: 1
    lambda_lpips: 1
